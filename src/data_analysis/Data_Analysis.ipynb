{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "This file will contain all initial analysis of the data and plot it.\n",
    "\n",
    "TODO: Chloe do this (https://github.com/vaquierm/RedditCommentTextClassification/issues/11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\chloe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'raw_data_dir_path' from 'src.main' (../..\\src\\main.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-286-668892a04838>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_raw_training_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mautomate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_feature_vectors\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconvert_all_raw_data_to_feature_vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\Univ\\McGill\\Fall2019\\COMP551\\RedditCommentTextClassification\\src\\automate\\create_feature_vectors.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmain\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mraw_data_dir_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabularies_dir_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprocessed_dir_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabularies_to_run\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_processing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdictionary\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDictionary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_raw_training_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mload_raw_test_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_processed_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Univ\\McGill\\Fall2019\\COMP551\\RedditCommentTextClassification\\src\\main.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautomate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_feature_vectors\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconvert_all_raw_data_to_feature_vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautomate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_vocabularies\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcreate_vocabularies\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautomate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidation_pipeline\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrun_validation_pipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Univ\\McGill\\Fall2019\\COMP551\\RedditCommentTextClassification\\src\\automate\\create_feature_vectors.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmain\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mraw_data_dir_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabularies_dir_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprocessed_dir_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabularies_to_run\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_processing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdictionary\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDictionary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_raw_training_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mload_raw_test_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_processed_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'raw_data_dir_path' from 'src.main' (../..\\src\\main.py)"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "nltk.download('punkt')\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../../')\n",
    "\n",
    "from utils import load_raw_training_data\n",
    "from automate.create_feature_vectors import convert_all_raw_data_to_feature_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(file_path):\n",
    "    print(\"Reading data files...\")\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "#todo: clean the data beforehand\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_most_common_words(data, top_nb = 5):\n",
    "    print(\"Computing most common words...\")\n",
    "    \n",
    "    fdist_allsubs = FreqDist()\n",
    "    list_subreddit_freq = []\n",
    "    subreddit_names = data.subreddits.unique()\n",
    "    \n",
    "    for subreddit in subreddit_names:\n",
    "        print(\"\\t\" + subreddit)\n",
    "        \n",
    "        # get data associated to subreddit\n",
    "        subreddit_data = data.loc[data[\"subreddits\"] == subreddit]\n",
    "        \n",
    "        # get frequency distribution of all words\n",
    "        subreddit_comments = nltk.tokenize.word_tokenize(subreddit_data[\"comments\"].str.cat(sep=\"\\n\"))\n",
    "        fdist_subreddit = nltk.FreqDist(subreddit_comments)\n",
    "        \n",
    "#         fdist_subreddit.plot(5, cumulative = False)\n",
    "#         plt.show()\n",
    "        \n",
    "        print(fdist_subreddit)\n",
    "        print(fdist_subreddit.most_common(top_nb))\n",
    "        list_subreddit_freq.append(fdist_subreddit)\n",
    "        fdist_allsubs += fdist_subreddit\n",
    "    \n",
    "    print(\"\\tall subreddits\")\n",
    "    print(fdist_allsubs)\n",
    "    print(fdist_allsubs.most_common(top_nb))\n",
    "    \n",
    "    i = 0\n",
    "    for subreddit_freq in list_subreddit_freq:\n",
    "        idtf(fdist_allsubs, subreddit_freq, subreddit_names[i])\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- IDF = nb of that word / total nb of that word\n",
    "- Scoring function - which words are the most related to a subreddit\n",
    "- Average number of words for comments\n",
    "- Average number of words for each subreddit\n",
    "- Average number of youtube links per comment in each subreddit category (probably high for like overwatch/league of legends)\n",
    "- Check if there are any unknown charachters (There could be japanese charachters in comments in the anime subreddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def scoring_function(data):\n",
    "#     todo: calculate scoring function based on the feature vectors\n",
    "#     corr, _ = spearmanr(data, data2)\n",
    "#     print('Spearmans correlation: %.3f' % corr)\n",
    "    corr = data.corr()\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(corr,cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    fig.colorbar(cax)\n",
    "    ticks = np.arange(0,len(data.columns),1)\n",
    "    ax.set_xticks(ticks)\n",
    "    plt.xticks(rotation=90)\n",
    "    ax.set_yticks(ticks)\n",
    "    ax.set_xticklabels(data.columns)\n",
    "    ax.set_yticklabels(data.columns)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idtf(fdist_all, fdist_subreddit, subreddit_name):\n",
    "    #using the frequency distribution we already have, find occurence of word in on subreddit / occurence of word in all\n",
    "    words = fdist_all.keys()\n",
    "    dictionary = {}\n",
    "    for word in words:\n",
    "        occurence_subreddit = fdist_subreddit[word]/fdist_all[word]\n",
    "        if (occurence_subreddit > 0.5):\n",
    "            dictionary[word] = occurence_subreddit\n",
    "#             print(\"The word \" + word + \" occurs quite often in \" + subreddit_name)\n",
    "    sorted_dic = sorted(dictionary.items(), key=lambda kv: kv[1])\n",
    "    print(\"The top 10 words for \" + subreddit_name + \": \\n\\t\")\n",
    "    print(sorted_dic[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_nb_words(data):\n",
    "    COUNT_COLUMN = \"count words\"\n",
    "    COUNT_YOUTUBE_COLUMN = \"count youtube\"\n",
    "    \n",
    "    print(\"Computing average number of words per comment...\")\n",
    "    data[COUNT_COLUMN] = data.apply(lambda row: len(row.comments), axis=1)\n",
    "    count_youtube_comment = 0\n",
    "    \n",
    "    for subreddit in data.subreddits.unique():\n",
    "        print(\"\\t\" + subreddit)\n",
    "        # get data associated to subreddit\n",
    "        subreddit_data = data.loc[data[\"subreddits\"] == subreddit]\n",
    "        \n",
    "        # get mean of count of words \n",
    "        subreddit_mean_words = subreddit_data[COUNT_COLUMN].mean()\n",
    "        print(subreddit_mean_words)\n",
    "        \n",
    "        # find the nb of youtube links and divide by nb of comments (if data is processed, can just get nb of \"youtube\")\n",
    "        subreddit_count_youtube_comment = subreddit_data[\"comments\"].str.count(\"youtube\").sum()\n",
    "        print(\"nb of youtube comment:\" + str(subreddit_count_youtube_comment))\n",
    "        #add to global youtube count\n",
    "        count_youtube_comment += subreddit_count_youtube_comment\n",
    "        \n",
    "    print(\"\\t all subreddits\")\n",
    "    # get mean of count of words \n",
    "    mean_words = data[COUNT_COLUMN].mean()\n",
    "    print(mean_words)\n",
    "    print(\"total nb of youtube comment:\" + str(count_youtube_comment))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_statistics(data):\n",
    "    print(\"Computing statistics...\")\n",
    "    \n",
    "#     calculate_most_common_words(data)\n",
    "#     calculate_avg_nb_words(data)\n",
    "    scoring_function(convert_all_raw_data_to_feature_vectors())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data files...\n",
      "Computing statistics...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'corr'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-275-dd293c81c9af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfetch_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../../data/raw_data/reddit_train.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcalculate_statistics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-274-c423b848d08e>\u001b[0m in \u001b[0;36mcalculate_statistics\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#     calculate_most_common_words(data)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#     calculate_avg_nb_words(data)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mscoring_function\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"comments\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-266-3160dd67eaeb>\u001b[0m in \u001b[0;36mscoring_function\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#     corr, _ = spearmanr(data, data2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#     print('Spearmans correlation: %.3f' % corr)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mcorr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m111\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'corr'"
     ]
    }
   ],
   "source": [
    "data = fetch_data(\"../../data/raw_data/reddit_train.csv\")\n",
    "calculate_statistics(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
