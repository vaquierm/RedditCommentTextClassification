{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "This file will contains all initial analysis of the data and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from config import processed_dir_path\n",
    "from data_processing.vocabulary import token_youtube_link, token_internet_link, token_emoticon_funny, compute_sentiment_of_comments\n",
    "from nltk.corpus import stopwords\n",
    "set(stopwords.words('english'))\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate_most_common_words\n",
    "Find the most recurrent words for each subreddit. The comments are cleaned beforehand (stopwords and punctuation removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_most_common_words(data, top_nb = 5):\n",
    "    print(\"Computing most common words...\")\n",
    "    \n",
    "    fdist_allsubs = FreqDist()\n",
    "    list_subreddit_freq = []\n",
    "    subreddit_names = data.subreddits.unique()\n",
    "    \n",
    "    for subreddit in subreddit_names:\n",
    "        print(\"\\t\" + subreddit)\n",
    "        \n",
    "        # get data associated to subreddit\n",
    "        subreddit_data = data.loc[data[\"subreddits\"] == subreddit]\n",
    "        \n",
    "        # tokenize\n",
    "        subreddit_comments = nltk.tokenize.word_tokenize(subreddit_data[\"comments\"].str.cat(sep=\"\\n\"))\n",
    "        # remove stopswords and punctuation\n",
    "        subreddit_comments = [word for word in subreddit_comments if (word not in stopwords.words('english') and word.isalpha())]\n",
    "        \n",
    "        # get frequency distribution of all words\n",
    "        fdist_subreddit = nltk.FreqDist(subreddit_comments)\n",
    "        \n",
    "        fdist_subreddit.plot(5, cumulative = False)\n",
    "        plt.show()\n",
    "        \n",
    "        print(fdist_subreddit)\n",
    "        print(fdist_subreddit.most_common(top_nb))\n",
    "        list_subreddit_freq.append(fdist_subreddit)\n",
    "        fdist_allsubs += fdist_subreddit\n",
    "    \n",
    "    print(\"\\tall subreddits\")\n",
    "    print(fdist_allsubs)\n",
    "    print(fdist_allsubs.most_common(top_nb))\n",
    "    \n",
    "    i = 0\n",
    "    for subreddit_freq in list_subreddit_freq:\n",
    "        idtf(fdist_allsubs, subreddit_freq, subreddit_names[i])\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDTF\n",
    "Using the frequency distribution we already have, find occurence of word in on subreddit / occurence of word in all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idtf(fdist_all, fdist_subreddit, subreddit_name):\n",
    "    words = fdist_all.keys()\n",
    "    dictionary = {}\n",
    "    for word in words:\n",
    "        occurence_subreddit = fdist_subreddit[word]/fdist_all[word]\n",
    "        if (occurence_subreddit > 0.5):\n",
    "            dictionary[word] = occurence_subreddit\n",
    "#             print(\"The word \" + word + \" occurs quite often in \" + subreddit_name)\n",
    "    sorted_dic = sorted(dictionary.items(), key=lambda kv: kv[1])\n",
    "    print(\"The top 10 words for \" + subreddit_name + \": \")\n",
    "    print(sorted_dic[-10:])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate_avg_nb_words\n",
    "Compute average amount of words per comment, and also prints the total number of youtube links in the entire subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_nb_words(data):\n",
    "    COUNT_COLUMN = \"count words\"\n",
    "    COUNT_WORD_LENGTH_COLUMNS = \"count word length\"\n",
    "    SENTIMENT = \"sentiment\"\n",
    "    \n",
    "    print(\"Computing average number of words per comment...\")\n",
    "    data[COUNT_COLUMN] = data.apply(lambda row: len(row.comments.split()), axis=1)\n",
    "    data[COUNT_WORD_LENGTH_COLUMNS] = data.apply(lambda row: len(row.comments.replace(' ',''))/row[COUNT_COLUMN], axis=1)\n",
    "    data[SENTIMENT] = compute_sentiment_of_comments(data[\"comments\"])\n",
    "    \n",
    "    count_youtubelink, count_internetlink, count_emoticon = 0,0,0\n",
    "    \n",
    "    for subreddit in data.subreddits.unique():\n",
    "        print(\"\\n\\n\\t\" + subreddit)\n",
    "        # get data associated to subreddit\n",
    "        subreddit_data = data.loc[data[\"subreddits\"] == subreddit]\n",
    "        \n",
    "        # get sentiment of each comment\n",
    "        subreddit_sentiment = subreddit_data[SENTIMENT].mean()\n",
    "        print(\"Average sentiment:\"+ str(subreddit_sentiment))\n",
    "        \n",
    "        # get mean of count of words \n",
    "        subreddit_mean_words = subreddit_data[COUNT_COLUMN].mean()\n",
    "        print(\"Average words/comment:\"+ str(subreddit_mean_words))\n",
    "        \n",
    "        # get mean length of all words\n",
    "        subreddit_mean_word_length = subreddit_data[COUNT_WORD_LENGTH_COLUMNS].mean()\n",
    "        print(\"Average length of words:\"+ str(subreddit_mean_word_length))\n",
    "        \n",
    "        # find the nb of youtube links\n",
    "        subreddit_count_youtubelink = subreddit_data[\"comments\"].str.count(token_youtube_link).sum()\n",
    "        print(\"nb of youtube links in \" + subreddit + \":\" + str(subreddit_count_youtubelink))\n",
    "        #add to global youtube count\n",
    "        count_youtubelink += subreddit_count_youtubelink\n",
    "        \n",
    "        # find the nb of internet links\n",
    "        subreddit_count_internetlink = subreddit_data[\"comments\"].str.count(token_internet_link).sum()\n",
    "        print(\"nb of internet links in \" + subreddit + \":\" + str(subreddit_count_internetlink))\n",
    "        #add to global youtube count\n",
    "        count_internetlink += subreddit_count_internetlink\n",
    "        \n",
    "        # find the nb of emoticons\n",
    "        subreddit_count_emoticon = subreddit_data[\"comments\"].str.count(token_emoticon_funny).sum()\n",
    "        print(\"nb of emoticons in\" + subreddit + \":\" + str(subreddit_count_emoticon))\n",
    "        #add to global youtube count\n",
    "        count_emoticon += subreddit_count_emoticon\n",
    "        \n",
    "    print(\"\\n\\n\\t all subreddits\")\n",
    "    subreddit_sentiment = data[SENTIMENT].mean()\n",
    "    print(\"Average sentiment:\" + str(subreddit_sentiment))\n",
    "    mean_words = data[COUNT_COLUMN].mean()\n",
    "    print(\"Average words/comment:\" + str(mean_words))\n",
    "    mean_words = data[COUNT_WORD_LENGTH_COLUMNS].mean()\n",
    "    print(\"Average length of word:\" + str(mean_words))\n",
    "    print(\"Total nb of youtube links:\" + str(count_youtubelink))\n",
    "    print(\"Total nb of internet links:\" + str(count_internetlink))\n",
    "    print(\"Total nb of emoticons:\" + str(count_emoticon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file_path = processed_dir_path + \"/LEMMA_train_clean.csv\"\n",
    "print(\"Reading data file...\")\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "print(\"Computing statistics...\") \n",
    "calculate_most_common_words(data)\n",
    "calculate_avg_nb_words(data)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
