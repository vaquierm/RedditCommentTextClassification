{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "This file will contains all initial analysis of the data and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from config import raw_data_dir_path\n",
    "from nltk.corpus import stopwords\n",
    "set(stopwords.words('english'))\n",
    "\n",
    "YOUTUBE_KEYWORD = \"youtubelink\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate_most_common_words\n",
    "Find the most recurrent words for each subreddit. The comments are cleaned beforehand (stopwords and punctuation removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_most_common_words(data, top_nb = 5):\n",
    "    print(\"Computing most common words...\")\n",
    "    \n",
    "    fdist_allsubs = FreqDist()\n",
    "    list_subreddit_freq = []\n",
    "    subreddit_names = data.subreddits.unique()\n",
    "    \n",
    "    for subreddit in subreddit_names:\n",
    "        print(\"\\t\" + subreddit)\n",
    "        \n",
    "        # get data associated to subreddit\n",
    "        subreddit_data = data.loc[data[\"subreddits\"] == subreddit]\n",
    "        \n",
    "        # tokenize\n",
    "        subreddit_comments = nltk.tokenize.word_tokenize(subreddit_data[\"comments\"].str.cat(sep=\"\\n\"))\n",
    "        # remove stopswords and punctuation\n",
    "        subreddit_comments = [word for word in subreddit_comments if (word not in stopwords.words('english') and word.isalpha())]\n",
    "        \n",
    "        # get frequency distribution of all words\n",
    "        fdist_subreddit = nltk.FreqDist(subreddit_comments)\n",
    "        \n",
    "        fdist_subreddit.plot(5, cumulative = False)\n",
    "        plt.show()\n",
    "        \n",
    "        print(fdist_subreddit)\n",
    "        print(fdist_subreddit.most_common(top_nb))\n",
    "        list_subreddit_freq.append(fdist_subreddit)\n",
    "        fdist_allsubs += fdist_subreddit\n",
    "    \n",
    "    print(\"\\tall subreddits\")\n",
    "    print(fdist_allsubs)\n",
    "    print(fdist_allsubs.most_common(top_nb))\n",
    "    \n",
    "    i = 0\n",
    "    for subreddit_freq in list_subreddit_freq:\n",
    "        idtf(fdist_allsubs, subreddit_freq, subreddit_names[i])\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDTF\n",
    "Using the frequency distribution we already have, find occurence of word in on subreddit / occurence of word in all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idtf(fdist_all, fdist_subreddit, subreddit_name):\n",
    "    words = fdist_all.keys()\n",
    "    dictionary = {}\n",
    "    for word in words:\n",
    "        occurence_subreddit = fdist_subreddit[word]/fdist_all[word]\n",
    "        if (occurence_subreddit > 0.5):\n",
    "            dictionary[word] = occurence_subreddit\n",
    "#             print(\"The word \" + word + \" occurs quite often in \" + subreddit_name)\n",
    "    sorted_dic = sorted(dictionary.items(), key=lambda kv: kv[1])\n",
    "    print(\"The top 10 words for \" + subreddit_name + \": \\n\\t\")\n",
    "    print(sorted_dic[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate_avg_nb_words\n",
    "Compute average amount of words per comment, and also prints the total number of youtube links in the entire subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_nb_words(data):\n",
    "    COUNT_COLUMN = \"count words\"\n",
    "    COUNT_YOUTUBE_COLUMN = \"count youtube\"\n",
    "    \n",
    "    print(\"Computing average number of words per comment...\")\n",
    "    data[COUNT_COLUMN] = data.apply(lambda row: len(row.comments), axis=1)\n",
    "    count_youtube_comment = 0\n",
    "    \n",
    "    for subreddit in data.subreddits.unique():\n",
    "        print(\"\\t\" + subreddit)\n",
    "        # get data associated to subreddit\n",
    "        subreddit_data = data.loc[data[\"subreddits\"] == subreddit]\n",
    "        \n",
    "        # get mean of count of words \n",
    "        subreddit_mean_words = subreddit_data[COUNT_COLUMN].mean()\n",
    "        print(subreddit_mean_words)\n",
    "        \n",
    "        # find the nb of youtube links and divide by nb of comments (if data is processed, can just get nb of \"youtube\")\n",
    "        subreddit_count_youtube_comment = subreddit_data[\"comments\"].str.count(YOUTUBE_KEYWORD).sum()\n",
    "        print(\"nb of youtube comment:\" + str(subreddit_count_youtube_comment))\n",
    "        #add to global youtube count\n",
    "        count_youtube_comment += subreddit_count_youtube_comment\n",
    "        \n",
    "    print(\"\\t all subreddits\")\n",
    "    # get mean of count of words \n",
    "    mean_words = data[COUNT_COLUMN].mean()\n",
    "    print(mean_words)\n",
    "    print(\"total nb of youtube comment:\" + str(count_youtube_comment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file_path = raw_data_dir_path + \"/LEMMA_train_raw_clean.csv\"\n",
    "print(\"Reading data file...\")\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "print(\"Computing statistics...\") \n",
    "calculate_most_common_words(data)\n",
    "calculate_avg_nb_words(data)\n",
    "\n",
    "\n",
    "#     TODO: scoring function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
